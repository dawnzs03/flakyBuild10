[[machine-learning-training-methods-logistic-regression]]
[.beta]
= Logistic regression

include::partial$/operations-reference/beta-note.adoc[]

Logistic regression is a fundamental supervised machine learning classification method.
This trains a model by minimizing a loss function which depends on a weight matrix and on the training data.
The loss can be minimized for example using gradient descent.
In GDS we use the Adam optimizer which is a gradient descent type algorithm.

The weights are in the form of a `[c,d]` sized matrix `W` and a bias vector `b` of length `c`, where `d` is the feature dimension and `c` is equal to the number of classes.
The loss function is then defined as:

`CE(softmax(Wx + b))`

where `CE` is the https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression[cross entropy loss], `softmax` is the https://en.wikipedia.org/wiki/Softmax_function[softmax function], and `x` is a feature vector training sample of length `d`.

To avoid overfitting one may also add a https://en.wikipedia.org/wiki/Regularization_(mathematics)[regularization] term to the loss.
Neo4j Graph Data Science supports the option of `l2` regularization which can be configured using the `penalty` parameter.


include::partial$/machine-learning/training-methods/gradient-descent-config-tuning.adoc[leveloffset =+ 1]

include::partial$/machine-learning/training-methods/penalty-config-tuning.adoc[leveloffset =+ 1]

include::partial$/machine-learning/training-methods/focus-weight-config-tuning.adoc[leveloffset =+ 1]
