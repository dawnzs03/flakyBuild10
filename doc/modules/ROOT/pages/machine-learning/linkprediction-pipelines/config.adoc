[[linkprediction-configure]]
[.beta]
= Configuring the pipeline
:description: This section describes configuration of Link prediction pipelines in the Neo4j Graph Data Science library.
:modelType: LinkPrediction
:entity: relationship
:max-trials: 10

include::partial$/operations-reference/beta-note.adoc[]

This page explains how to create and configure a link prediction pipeline.


[[linkprediction-creating-a-pipeline]]
== Creating a pipeline

The first step of building a new pipeline is to create one using `gds.beta.pipeline.linkPrediction.create`.
This stores a trainable pipeline object in the pipeline catalog of type `Link prediction training pipeline`.
This represents a configurable pipeline that can later be invoked for training, which in turn creates a trained pipeline.
The latter is also a model which is stored in the catalog with type `LinkPrediction`.


=== Syntax

[.pipeline-create-syntax]
--
.Create pipeline syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.create(
  pipelineName: String
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureSteps: List of Map,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type   | Description
| pipelineName    | String  | The name of the created pipeline.
|===

include::partial$/machine-learning/linkprediction-pipeline/pipelineInfoResult.adoc[]
--


=== Example

[role=query-example,group=lp]
--
.The following will create a pipeline:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.create('pipe')
----

.Results
[opts="header",cols="1,1,1,1,1,1"]
|===
| name     | nodePropertySteps | featureSteps | splitConfig | autoTuningConfig | parameterSpace
| "pipe"   | []                | []
| {negativeSamplingRatio=1.0, testFraction=0.1, trainFraction=0.1, validationFolds=3}
| {maxTrials={max-trials}}
| {LogisticRegression=[], MultilayerPerceptron=[], RandomForest=[]}
|===
--

This shows that the newly created pipeline does not contain any steps yet, and has defaults for the split and train parameters.


[[linkprediction-adding-node-properties]]
== Adding node properties

A link prediction pipeline can execute one or several GDS algorithms in mutate mode that create node properties in the projected graph.
Such steps producing node properties can be chained one after another and created properties can also be used to xref:machine-learning/linkprediction-pipelines/config.adoc#linkprediction-adding-features[add features].
Moreover, the node property steps that are added to the pipeline will be executed both when xref:machine-learning/linkprediction-pipelines/training.adoc[training] a pipeline and when the trained model is xref:machine-learning/linkprediction-pipelines/predict.adoc[applied for prediction].

The name of the procedure that should be added can be a fully qualified GDS procedure name ending with `.mutate`.
The ending `.mutate` may be omitted and one may also use shorthand forms such as `beta.node2vec` instead of `gds.beta.node2vec.mutate`.
But please note that tier qualification (in this case `beta`) must still be given as part of the name.

For example, xref:machine-learning/pre-processing/index.adoc[pre-processing algorithms] can be used as node property steps.


=== Syntax

[.pipeline-add-node-property-syntax]
--
.Add node property syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.addNodeProperty(
  pipelineName: String,
  procedureName: String,
  procedureConfiguration: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureSteps: List of Map,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name                      | Type    | Description
| pipelineName              | String  | The name of the pipeline.
| procedureName             | String  | The name of the procedure to be added to the pipeline.
| procedureConfiguration    | Map     | The map used to generate the configuration of the procedure. It includes procedure specific configurations except `nodeLabels` and `relationshipTypes`. It can optionally contain parameters in table below.
|===

include::partial$/machine-learning/node-property-step-context-config.adoc[]


include::partial$/machine-learning/linkprediction-pipeline/pipelineInfoResult.adoc[]
--
=== Example

[role=query-example,group=lp]
--
.The following will add a node property step to the pipeline:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.addNodeProperty('pipe', 'fastRP', {
  mutateProperty: 'embedding',
  embeddingDimension: 256,
  randomSeed: 42
})
----

.Results
[opts="header",cols="1,1,1,1,1,1"]
|===
| name     | nodePropertySteps | featureSteps | splitConfig | autoTuningConfig | parameterSpace
| "pipe"   | [{name=gds.fastRP.mutate, config={randomSeed=42, contextRelationshipTypes=[], embeddingDimension=256, contextNodeLabels=[], mutateProperty=embedding}}]
| []
| {negativeSamplingRatio=1.0, testFraction=0.1, trainFraction=0.1, validationFolds=3}
| {maxTrials={max-trials}}
| {LogisticRegression=[], MultilayerPerceptron=[], RandomForest=[]}
|===

The pipeline will now execute the xref:machine-learning/node-embeddings/fastrp.adoc[fastRP algorithm] in mutate mode both before xref:machine-learning/linkprediction-pipelines/training.adoc[training] a model, and when the trained model is xref:machine-learning/linkprediction-pipelines/predict.adoc[applied for prediction].
This ensures the `embedding` property can be used as an input for link features.
--


[[linkprediction-adding-features]]
== Adding link features

A Link Prediction pipeline executes a sequence of steps to compute the features used by a machine learning model.
A feature step computes a vector of features for given node pairs.
For each node pair, the results are concatenated into a single _link feature vector_.
The order of the features in the link feature vector follows the order of the feature steps.
Like with node property steps, the feature steps are also executed both at xref:machine-learning/linkprediction-pipelines/training.adoc[training] and xref:machine-learning/linkprediction-pipelines/predict.adoc[prediction] time.
The supported methods for obtaining features are described xref:machine-learning/linkprediction-pipelines/config.adoc#linkprediction-supported-features[below].


=== Syntax

[.pipeline-add-feature-syntax]
--
.Adding a link feature to a pipeline
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.addFeature(
  pipelineName: String,
  featureType: String,
  configuration: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureSteps: List of Map,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name                   | Type    | Description
| pipelineName           | String  | The name of the pipeline.
| featureType            | String  | The featureType determines the method used for computing the link feature. See xref:machine-learning/linkprediction-pipelines/config.adoc#linkprediction-supported-features[supported types].
| configuration          | Map     | Configuration for adding the link feature.
|===

.Configuration
[opts="header",cols="1,1,1,4"]
|===
| Name              | Type              | Default | Description
| nodeProperties    | List of String    | no      | The names of the node properties that should be used as input.
|===

include::partial$/machine-learning/linkprediction-pipeline/pipelineInfoResult.adoc[]
--


[[linkprediction-supported-features]]
=== Supported feature types

A feature step can use node properties that exist in the input graph or are added by the pipeline.
For each node in each potential link, the values of `nodeProperties` are concatenated, in the configured order, into a vector _f_.
That is, for each potential link the feature vector for the source node, image:equations/linkprediction/linkprediction1.svg[s equals s1 comma s2 comma dot dot dot s d], is combined with the one for the target node, image:equations/linkprediction/linkprediction2.svg[s equals t1 comma t2 comma dot dot dot t d], into a single feature vector _f_.

The supported types of features can then be described as follows:

.Supported feature types
[opts="header",cols="1,4"]
|===
| Feature Type           | Formula / Description
| L2                     | image:equations/linkprediction/linkprediction3.svg[f equals vector of s1 minus t1 squared comma s2 minus t2 squared comma dot dot dot comma s d minus t d squared]
| HADAMARD               | image:equations/linkprediction/linkprediction4.svg[f equals vector of s1 dot t1 comma s2 dot t2 comma dot dot dot comma s d dot t d]
| COSINE                 | image:equations/linkprediction/linkprediction5.svg[f equals sum of i from 1 to d of s i t i divided by square root of sum of i from 1 to d of s i squared times square root of sum of i from 1 to d of t i squared]
| SAME_CATEGORY          | The feature is `1` if the category value of source and target are the same, otherwise its `0`. Similar to xref:alpha-algorithms/same-community.adoc#algorithms-linkprediction-same-community-sample[Same Community].
|===


=== Example

[role=query-example,group=lp]
--
.The following will add a feature step to the pipeline:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.addFeature('pipe', 'hadamard', {
  nodeProperties: ['embedding', 'age']
}) YIELD featureSteps
----

.Results
[opts="header",cols="1"]
|===
| featureSteps
| [{name=HADAMARD, config={nodeProperties=[embedding, age]}}]
|===

When executing the pipeline, the `nodeProperties` must be either present in the input graph, or created by a previous node property step.
For example, the `embedding` property could be created by the previous example, and we expect `age` to already be present in the in-memory graph used as input, at train and predict time.
--


[[linkprediction-configure-splits]]
== Configuring the relationship splits

Link Prediction training pipelines manage splitting the relationships into several sets and add sampled negative relationships to some of these sets.
Configuring the splitting is optional, and if omitted, splitting will be done using default settings.

The splitting configuration of a pipeline can be inspected by using `gds.beta.model.list` and possibly only yielding `splitConfig`.

The splitting of relationships proceeds internally in the following steps:

1. The graph is filtered according to specified `sourceNodeLabel`, `targetNodeLabel` and `targetRelationshipType`, which are configured at train time.
2. The relationships remaining after filtering we call _positive_, they are split into `test`, `train` and `feature-input` sets.
* The `test` set contains a `testFraction` fraction of the positive relationships.
The remaining positive relationships are referred to as the `testComplement` set. [[def-test-complement]]
* The `train` set contains `trainFraction` of the `testComplement` set.
* The `feature-input` set contains the rest.
3. Random negative relationships, which conform to the `sourceNodeLabel` and `targetNodeLabel` filter, are added to the `test` and `train` sets.
* The number of negative relationships in each set is the number of positive ones multiplied by the `negativeSamplingRatio`.
* The negative relationships do not coincide with positive relationships.
* If `negativeRelationshipType` is specified, then instead of sampling, all relationships of this type in the graph are partitioned according to the `test` and `train` set size ratio and added as negative relationships.
All relationships of `negativeRelationshipType` must also conform to the `sourceNodeLabel` and `targetNodeLabel` filter.


The positive and negative relationships are given relationship weights of `1.0` and `0.0` respectively so that they can be distinguished.

The `train` and `test` relationship sets are used for:

* determining the label (positive or negative) for each training or test example
* identifying the node pair for which link features are to be computed

However, they are not used by the algorithms run in the node property steps.
The reason for this is that otherwise the model would use the prediction target (existence of a relationship) as a feature.

Each node property step uses a feature-input _graph_.
The `feature-input` graph has nodes with `sourceNodeLabel`, `targetNodeLabel` and `contextNodeLabels` and the relationships from the `feature-input` set plus those of `contextRelationshipTypes`.
This graph is used for computing node properties and features which depend on node properties.
The node properties generated in the `feature-input` graph are used in training and testing.


=== Syntax

[.pipeline-configure-split-syntax]
--
.Configure the relationship split syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.configureSplit(
  pipelineName: String,
  configuration: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureSteps: List of Map,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type    | Description
| pipelineName    | String  | The name of the pipeline.
| configuration   | Map     | Configuration for splitting the relationships.
|===

.Configuration
[opts="header",cols="1,1,1,4"]
|===
| Name                  | Type    | Default | Description
| validationFolds       | Integer | 3       | Number of divisions of the training graph used during xref:machine-learning/linkprediction-pipelines/training.adoc[model selection].
| testFraction          | Double  | 0.1     | Fraction of the graph reserved for testing. Must be in the range (0, 1).
| trainFraction         | Double  | 0.1     | Fraction of the <<def-test-complement,test-complement set>> reserved for training. Must be in the range (0, 1).
| negativeSamplingRatio | Double  | 1.0     | The desired ratio of negative to positive samples in the test and train set. More details xref:machine-learning/linkprediction-pipelines/theory.adoc#linkprediction-pipelines-classimbalance[here]. It is a mutually exclusive parameter with `negativeRelationshipType`.
| negativeRelationshipType | String | n/a | Specifies which relationships should be used as negative relationships, added to the `test` and `train` sets. It is a mutually exclusive parameter with `negativeSamplingRatio`.
|===

include::partial$/machine-learning/linkprediction-pipeline/pipelineInfoResult.adoc[]
--


=== Example

[role=query-example,group=lp]
--
.The following will configure the splitting of the pipeline:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.configureSplit('pipe', {
  testFraction: 0.25,
  trainFraction: 0.6,
  validationFolds: 3
})
YIELD splitConfig
----

.Results
[opts="header",cols="1"]
|===
| splitConfig
| {negativeSamplingRatio=1.0, testFraction=0.25, trainFraction=0.6, validationFolds=3}
|===

We now reconfigured the splitting of the pipeline, which will be applied during xref:machine-learning/linkprediction-pipelines/training.adoc[training].
--


As an example, consider a graph with nodes 'Person' and 'City' and relationships 'KNOWS', 'BORN' and 'LIVES'.
Please note that this is the same example as in xref:machine-learning/linkprediction-pipelines/training.adoc#linkprediction-pipelines-train-example[Training the pipeline].

.Full example graph
image::example-graphs/link-prediction.svg[Visualization of the example graph,align="center"]

Suppose we filter by `sourceNodeLabel` and `targetNodeLabel` being `Person` and `targetRelationshipType` being `KNOWS`.
The filtered graph looks like the following:

.Filtered graph
image::example-graphs/lp-split.png[example graph for LP split,align="center"]

The filtered graph has 12 relationships.
If we configure split with `testFraction` 0.25 and `negativeSamplingRatio` 1, it randomly picks `12 * 0.25 = 3` positive relationships plus `1 * 3 = 3` negative relationship as the `test` set.

Then if `trainFraction` is 0.6 and `negativeSamplingRatio` 1, it randomly picks `9 * 0.6 = 5.4 ≈ 5` positive relationships plus `1 * 5 = 5` negative relationship as the `train` set.

The remaining `12 * (1 - 0.25) * (1 - 0.6) = 3.6 ≈ 4` relationships in yellow is the `feature-input` set.

.Positive and negative relationships for each set according to the split. The `test` set is in blue, `train` set in red and `feature-input` set in yellow. Dashed lines represent negative relationships.
image::example-graphs/lp-split-1.png[example graph for LP split,align="center"]

Suppose for example a node property step is added with `contextNodeLabel` `City` and `contextRelationshipType` `BORN`.
Then the `feature-input` graph for that step would be:

.Feature-input graph. The `feature-input` set is in yellow.
image::example-graphs/lp-split-2.png[example graph for LP split,align="center"]

[[linkprediction-adding-model-candidates]]
== Adding model candidates

A pipeline contains a collection of configurations for model candidates which is initially empty.
This collection is called the _parameter space_.
Each model candidate configuration contains either fixed values or ranges for training parameters.
When a range is present, values from the range are determined automatically by an auto-tuning algorithm, see xref:machine-learning/auto-tuning.adoc[Auto-tuning].
One or more model configurations must be added to the _parameter space_ of the training pipeline, using one of the following procedures:

* `gds.beta.pipeline.linkPrediction.addLogisticRegression`
* `gds.beta.pipeline.linkPrediction.addRandomForest`
* `gds.alpha.pipeline.linkPrediction.addMLP`

For information about the available training methods in GDS, logistic regression, random forest and multilayer perceptron, see xref:machine-learning/training-methods/index.adoc[Training methods].

In xref:machine-learning/node-property-prediction/nodeclassification-pipelines/training.adoc[Training the pipeline], we explain further how the configured model candidates are trained, evaluated and compared.

The parameter space of a pipeline can be inspected using `gds.beta.model.list` and optionally yielding only `parameterSpace`.

[NOTE]
====
At least one model candidate must be added to the pipeline before training it.
====


=== Syntax


[.tabbed-example, caption = ]
====
[.include-with-logistic-regression]
======
[.pipeline-add-lr-syntax]
--
.Configure the train parameters syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.addLogisticRegression(
  pipelineName: String,
  config: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureSteps: List of Map,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type        | Description
| pipelineName    | String      | The name of the pipeline.
| config          | Map         | The logistic regression config for a model candidate. The allowed parameters for a model are defined in the next table.
|===

.Logistic regression configuration
[opts="header",cols="4m,5,2m,2,8"]
|===
| Name        | Type  | Default | Optional | Description
include::partial$/machine-learning/training-methods/logisticRegressionConfig.adoc[]
| classWeights | List of Float | [1.0, 1.0]     | yes      | Weights for each class in loss function. The list must have length 2. The first weight is for negative examples (missing relationships), and the second for positive examples (actual relationships).
|===

include::partial$/machine-learning/linkprediction-pipeline/pipelineInfoResult.adoc[]
--
======

[.include-with-random-forest]
======
[.pipeline-add-rf-syntax]
--
.Configure the train parameters syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.addRandomForest(
  pipelineName: String,
  config: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureSteps: List of Map,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type        | Description
| pipelineName    | String      | The name of the pipeline.
| config          | Map         | The random forest config for a model candidate. The allowed parameters for a model are defined in the next table.
|===

include::partial$/machine-learning/training-methods/random-forest-classification-config.adoc[]

include::partial$/machine-learning/linkprediction-pipeline/pipelineInfoResult.adoc[]
--
======

[.include-with-multilayer-perceptron]
======
[.pipeline-add-mlp-syntax]
--
.Configure the train parameters syntax
[source, cypher, role=noplay]
----
CALL gds.alpha.pipeline.linkPrediction.addMLP(
  pipelineName: String,
  config: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureSteps: List of Map,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type        | Description
| pipelineName    | String      | The name of the pipeline.
| config          | Map         | The multilayer perceptron config for a model candidate. The allowed parameters for a model are defined in the next table.
|===

.Multilayer Perceptron Classification configuration
[opts="header",cols="4,5,2m,2,8"]
|===
| Name                | Type                            | Default         | Optional | Description
include::partial$/machine-learning/training-methods/mlp-classification-config.adoc[]
| classWeights | List of Float | [1.0, 1.0]     | yes      | Weights for each class in cross-entropy loss. The list must have length 2. The first weight is for negative examples (missing relationships), and the second for positive examples (actual relationships).
|===

include::partial$/machine-learning/linkprediction-pipeline/pipelineInfoResult.adoc[]
--
======
====


=== Example

We can add multiple model candidates to our pipeline.

[source, cypher, role=noplay query-example, no-result=true, group=lp]
.The following will add a logistic regression model with default configuration:
--
CALL gds.beta.pipeline.linkPrediction.addLogisticRegression('pipe')
YIELD parameterSpace
--

[source, cypher, role=noplay query-example, no-result=true, group=lp]
.The following will add a random forest model:
--
CALL gds.beta.pipeline.linkPrediction.addRandomForest('pipe', {numberOfDecisionTrees: 10})
YIELD parameterSpace
--

[source, cypher, role=noplay query-example, no-result=true, group=lp]
.The following will add a configured multilayer perceptron model with class weighted focal loss and ranged parameters:
--
CALL gds.alpha.pipeline.linkPrediction.addMLP('pipe',
{hiddenLayerSizes: [4, 2], penalty: 0.5, patience: 2, classWeights: [0.55, 0.45], focusWeight: {range: [0.0, 0.1]}})
YIELD parameterSpace
--

[role=query-example,group=lp]
--
.The following will add a logistic regression model with a range parameter:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.addLogisticRegression('pipe', {maxEpochs: 500, penalty: {range: [1e-4, 1e2]}})
YIELD parameterSpace
RETURN parameterSpace.RandomForest AS randomForestSpace, parameterSpace.LogisticRegression AS logisticRegressionSpace, parameterSpace.MultilayerPerceptron AS MultilayerPerceptronSpace
----

.Results
[opts="header",cols="1, 1, 1"]
|===
| randomForestSpace | logisticRegressionSpace | MultilayerPerceptronSpace
| [{maxDepth=2147483647, minLeafSize=1, criterion=GINI, minSplitSize=2, numberOfDecisionTrees=10, methodName=RandomForest, numberOfSamplesRatio=1.0}] | [{maxEpochs=100, minEpochs=1, classWeights=[], penalty=0.0, patience=1, methodName=LogisticRegression, focusWeight=0.0, batchSize=100, tolerance=0.001, learningRate=0.001}, {maxEpochs=500, minEpochs=1, classWeights=[], penalty={range=[1.0E-4, 100.0]}, patience=1, methodName=LogisticRegression, focusWeight=0.0, batchSize=100, tolerance=0.001, learningRate=0.001}] | [{maxEpochs=100, minEpochs=1, classWeights=[0.55, 0.45], penalty=0.5, patience=2, methodName=MultilayerPerceptron, focusWeight={range=[0.0, 0.1]}, hiddenLayerSizes=[4, 2], batchSize=100, tolerance=0.001, learningRate=0.001}]
|===
--

The `parameterSpace` in the pipeline now contains the four different model candidates, expanded with the default values.
Each specified model candidate will be tried out during the model selection in xref:machine-learning/linkprediction-pipelines/training.adoc[training].

[NOTE]
====
These are somewhat naive examples of how to add and configure model candidates.
Please see xref:machine-learning/training-methods/index.adoc[Training methods] for more information on how to tune the configuration parameters of each method.
====


[[linkprediction-configure-auto-tuning]]
== Configuring Auto-tuning

In order to find good models, the pipeline supports automatically tuning the parameters of the training algorithm.
Optionally, the procedure described below can be used to configure the auto-tuning behavior.
Otherwise, default auto-tuning configuration is used.
Currently, it is only possible to configure the maximum number trials of hyper-parameter settings which are evaluated.


=== Syntax

[.pipeline-configure-auto-tuning-syntax]
--
.Configuring auto-tuning syntax
[source, cypher, role=noplay]
----
CALL gds.alpha.pipeline.linkPrediction.configureAutoTuning(
  pipelineName: String,
  configuration: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureSteps: List of Map,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type    | Description
| pipelineName    | String  | The name of the created pipeline.
| configuration   | Map     | The configuration for auto-tuning.
|===

.Configuration
[opts="header",cols="1,1,1,4"]
|===
| Name            | Type    | Default      | Description
| maxTrials       | Integer | {max-trials} | The value of `maxTrials` determines the maximum allowed model candidates that should be evaluated and compared when training the pipeline. If no ranges are present in the parameter space, `maxTrials` is ignored and the each model candidate in the parameter space is evaluated.
|===

include::partial$/machine-learning/linkprediction-pipeline/pipelineInfoResult.adoc[]
--


=== Example

[role=query-example,group=lp]
--
.The following will configure the maximum trials for the auto-tuning:
[source, cypher, role=noplay]
----
CALL gds.alpha.pipeline.linkPrediction.configureAutoTuning('pipe', {
  maxTrials: 2
}) YIELD autoTuningConfig
----

.Results
[opts="header",cols="1"]
|===
| autoTuningConfig
| {maxTrials=2}
|===

We now reconfigured the auto-tuning to try out at most 2 model candidates during xref:machine-learning/linkprediction-pipelines/training.adoc[training].
--
