[[nodeclassification-pipelines-config]]
[.beta]
= Configuring the pipeline
:description: This section describes configuration of Node classification pipelines in the Neo4j Graph Data Science library.
:modelType: NodeClassification
:entity: node
:max-trials: 10

include::partial$/operations-reference/beta-note.adoc[]

This page explains how to create and configure a node classification pipeline.


[[nodeclassification-creating-a-pipeline]]
== Creating a pipeline

The first step of building a new pipeline is to create one using `gds.beta.pipeline.nodeClassification.create`.
This stores a trainable pipeline object in the pipeline catalog of type `Node classification training pipeline`.
This represents a configurable pipeline that can later be invoked for training, which in turn creates a classification model.
The latter is also a model which is stored in the catalog with type `NodeClassification`.


=== Syntax

[.pipeline-create-syntax]
--
.Create pipeline syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.create(
  pipelineName: String
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type    | Description
| pipelineName    | String  | The name of the created pipeline.
|===

include::partial$/machine-learning/node-property-prediction/pipelineInfoResult.adoc[]
--


=== Example

[role=query-example,group=nc]
--
.The following will create a pipeline:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.create('pipe')
----

.Results
[opts="header",cols="1,1,1,1,1,1"]
|===
| name     | nodePropertySteps | featureProperties | splitConfig | autoTuningConfig | parameterSpace
| "pipe"   | []                | []
| {testFraction=0.3, validationFolds=3}
| {maxTrials={max-trials}}
| {LogisticRegression=[], MultilayerPerceptron=[], RandomForest=[]}
|===
--

This shows that the newly created pipeline does not contain any steps yet, and has defaults for the split and train parameters.


[[nodeclassification-pipelines-adding-node-properties]]
== Adding node properties

A node classification pipeline can execute one or several GDS algorithms in mutate mode that create node properties in the in-memory graph.
Such steps producing node properties can be chained one after another and created properties can later be used as xref:machine-learning/node-property-prediction/nodeclassification-pipelines/config.adoc#nodeclassification-pipelines-adding-features[features].
Moreover, the node property steps that are added to the training pipeline will be executed both when xref:machine-learning/node-property-prediction/nodeclassification-pipelines/training.adoc[training] a model and when the classification pipeline is xref:machine-learning/node-property-prediction/nodeclassification-pipelines/predict.adoc[applied for classification].

The name of the procedure that should be added can be a fully qualified GDS procedure name ending with `.mutate`.
The ending `.mutate` may be omitted and one may also use shorthand forms such as `beta.node2vec` instead of `gds.beta.node2vec.mutate`.
But please note that tier qualification (in this case `beta`) must still be given as part of the name.

For example, xref:machine-learning/pre-processing/index.adoc[pre-processing algorithms] can be used as node property steps.


=== Syntax

[.pipeline-add-node-property-syntax]
--
.Add node property syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.addNodeProperty(
  pipelineName: String,
  procedureName: String,
  procedureConfiguration: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name                      | Type    | Description
| pipelineName              | String  | The name of the pipeline.
| procedureName             | String  | The name of the procedure to be added to the pipeline.
| procedureConfiguration    | Map     | The map used to generate the configuration of the procedure. It includes procedure specific configurations except `nodeLabels` and `relationshipTypes`. It can optionally contain parameters in table below.
|===

include::partial$/machine-learning/node-property-step-context-config.adoc[]

include::partial$/machine-learning/node-property-prediction/pipelineInfoResult.adoc[]
--


=== Example

[role=query-example,group=nc]
--
.The following will add a node property step to the pipeline. Here we assume that the input graph contains a property `sizePerStory`.
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.addNodeProperty('pipe', 'scaleProperties', {
  nodeProperties: 'sizePerStory',
  scaler: 'Mean',
  mutateProperty:'scaledSizes'
})
YIELD name, nodePropertySteps
----

.Results
[opts="header",cols="1,9"]
|===
| name     | nodePropertySteps
| "pipe"   | [{name=gds.scaleProperties.mutate, config={scaler=Mean, contextRelationshipTypes=[], contextNodeLabels=[], mutateProperty=scaledSizes, nodeProperties=sizePerStory}}]
|===

The `scaledSizes` property can be later used as a feature.
--


[[nodeclassification-pipelines-adding-features]]
== Adding features

A Node Classification Pipeline allows you to select a subset of the available node properties to be used as features for the machine learning model.
When executing the pipeline, the selected `nodeProperties` must be either present in the input graph, or created by a previous node property step.


=== Syntax

[.pipeline-add-feature-syntax]
--
.Adding a feature to a pipeline syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.selectFeatures(
  pipelineName: String,
  nodeProperties: List or String
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name                   | Type            | Description
| pipelineName           | String          | The name of the pipeline.
| nodeProperties         | List or String  | Node properties to use as model features.
|===

include::partial$/machine-learning/node-property-prediction/pipelineInfoResult.adoc[]
--


=== Example

[role=query-example,group=nc]
--
.The following will select features for the pipeline.
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.selectFeatures('pipe', ['scaledSizes', 'sizePerStory'])
YIELD name, featureProperties
----

.Results
[opts="header",cols="1,1"]
|===
| name     | featureProperties
| "pipe"   | [scaledSizes, sizePerStory]
|===

Here we assume that the input graph contains a property `sizePerStory` and `scaledSizes` was created in a `nodePropertyStep`.
--


[[nodeclassification-pipelines-configure-splits]]
== Configuring the node splits
:pipeline-type: Classification
:parameterspace-link: nodeclassification-pipelines-adding-model-candidates

include::partial$/machine-learning/node-property-prediction/nodeSplit-intro.adoc[]

=== Syntax

[.pipeline-configure-split-syntax]
--
.Configure the node split syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.configureSplit(
  pipelineName: String,
  configuration: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of Strings,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: List of Map
----

include::partial$/machine-learning/node-property-prediction/nodeSplit-config.adoc[]

include::partial$/machine-learning/node-property-prediction/pipelineInfoResult.adoc[]
--


=== Example

[role=query-example,group=nc]
--
.The following will configure the splitting of the pipeline:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.configureSplit('pipe', {
 testFraction: 0.2,
  validationFolds: 5
})
YIELD splitConfig
----

.Results
[opts="header",cols="1"]
|===
| splitConfig
| {testFraction=0.2, validationFolds=5}
|===

We now reconfigured the splitting of the pipeline, which will be applied during xref:machine-learning/node-property-prediction/nodeclassification-pipelines/training.adoc[training].
--


[[nodeclassification-pipelines-adding-model-candidates]]
== Adding model candidates

A pipeline contains a collection of configurations for model candidates which is initially empty.
This collection is called the _parameter space_.
Each model candidate configuration contains either fixed values or ranges for training parameters.
When a range is present, values from the range are determined automatically by an auto-tuning algorithm, see xref:machine-learning/auto-tuning.adoc[Auto-tuning].
One or more model configurations must be added to the _parameter space_ of the training pipeline, using one of the following procedures:

* `gds.beta.pipeline.nodeClassification.addLogisticRegression`
* `gds.beta.pipeline.nodeClassification.addRandomForest`
* `gds.alpha.pipeline.nodeClassification.addMLP`

For information about the available training methods in GDS, logistic regression, random forest and multilayer perceptron, see xref:machine-learning/training-methods/index.adoc[Training methods].

In xref:machine-learning/node-property-prediction/nodeclassification-pipelines/training.adoc[Training the pipeline], we explain further how the configured model candidates are trained, evaluated and compared.

The parameter space of a pipeline can be inspected using `gds.beta.model.list` and optionally yielding only `parameterSpace`.

[NOTE]
====
At least one model candidate must be added to the pipeline before training it.
====


=== Syntax

[.tabbed-example, caption = ]
====
[.include-with-logistic-regression]
======
[.pipeline-add-lr-syntax]
--
.Configure the train parameters syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.addLogisticRegression(
  pipelineName: String,
  config: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type        | Description
| pipelineName    | String      | The name of the pipeline.
| config          | Map         | The logistic regression config for a potential model. The allowed parameters for a model are defined in the next table.
|===

.Logistic regression configuration
[opts="header",cols="4m,5,2m,2,8"]
|===
| Name        | Type  | Default | Optional | Description
include::partial$/machine-learning/training-methods/logisticRegressionConfig.adoc[]
| classWeights | List of Float | List of 1.0     | yes      | Weights for each class in loss function. The `i^th^` weight is for the `i^th^` class (when ordering the classes by their integer values). The list must have length equal to the number of classes.
|===

include::partial$/machine-learning/node-property-prediction/pipelineInfoResult.adoc[]
--
======

[.include-with-random-forest]
======

[.pipeline-add-rf-syntax]
--
.Configure the train parameters syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.addRandomForest(
  pipelineName: String,
  config: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type        | Description
| pipelineName    | String      | The name of the pipeline.
| config          | Map         | The random forest config for a potential model. The allowed parameters for a model are defined in the next table.
|===

include::partial$/machine-learning/training-methods/random-forest-classification-config.adoc[]

include::partial$/machine-learning/node-property-prediction/pipelineInfoResult.adoc[]
--
======

[.include-with-multilayer-perceptron]
======

[.pipeline-add-mlp-syntax]
--
.Configure the train parameters syntax
[source, cypher, role=noplay]
----
CALL gds.alpha.pipeline.nodeClassification.addMLP(
  pipelineName: String,
  config: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type        | Description
| pipelineName    | String      | The name of the pipeline.
| config          | Map         | The multilayer perceptron config for a potential model. The allowed parameters for a model are defined in the next table.
|===

.Multilayer Perceptron Classification configuration
[opts="header",cols="4,5,2m,2,8"]
|===
| Name                | Type                            | Default         | Optional | Description
include::partial$/machine-learning/training-methods/mlp-classification-config.adoc[]
| classWeights | List of Float | List of 1.0     | yes      | Weights for each class in cross-entropy loss. The `i^th^` weight is for the `i^th^` class (when ordering the classes by their integer values). The list must have length equal to the number of classes.
|===

include::partial$/machine-learning/node-property-prediction/pipelineInfoResult.adoc[]
--
======
====


=== Example

We can add multiple model candidates to our pipeline.

[source, cypher, role=noplay query-example, no-result=true, group=nc]
.The following will add a logistic regression model with default configuration:
--
CALL gds.beta.pipeline.nodeClassification.addLogisticRegression('pipe')
YIELD parameterSpace
--

[source, cypher, role=noplay query-example, no-result=true, group=nc]
.The following will add a random forest model:
--
CALL gds.beta.pipeline.nodeClassification.addRandomForest('pipe', {numberOfDecisionTrees: 5})
YIELD parameterSpace
--

[source, cypher, role=noplay query-example, no-result=true, group=nc]
.The following will add a multilayer perceptron model with class weighted focal loss:
--
CALL gds.alpha.pipeline.nodeClassification.addMLP('pipe', {classWeights: [0.4,0.3,0.3], focusWeight: 0.5})
YIELD parameterSpace
--

[role=query-example,group=nc]
--
.The following will add a logistic regression model with a range parameter:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.addLogisticRegression('pipe', {maxEpochs: 500, penalty: {range: [1e-4, 1e2]}})
YIELD parameterSpace
RETURN parameterSpace.RandomForest AS randomForestSpace, parameterSpace.LogisticRegression AS logisticRegressionSpace, parameterSpace.MultilayerPerceptron AS MultilayerPerceptronSpace
----

.Results
[opts="header",cols="1, 1, 1"]
|===
| randomForestSpace | logisticRegressionSpace | MultilayerPerceptronSpace
| [{maxDepth=2147483647, minLeafSize=1, criterion=GINI, minSplitSize=2, numberOfDecisionTrees=5, methodName=RandomForest, numberOfSamplesRatio=1.0}] | [{maxEpochs=100, minEpochs=1, classWeights=[], penalty=0.0, patience=1, methodName=LogisticRegression, focusWeight=0.0, batchSize=100, tolerance=0.001, learningRate=0.001}, {maxEpochs=500, minEpochs=1, classWeights=[], penalty={range=[1.0E-4, 100.0]}, patience=1, methodName=LogisticRegression, focusWeight=0.0, batchSize=100, tolerance=0.001, learningRate=0.001}] | [{maxEpochs=100, minEpochs=1, classWeights=[0.4, 0.3, 0.3], penalty=0.0, patience=1, methodName=MultilayerPerceptron, focusWeight=0.5, hiddenLayerSizes=[100], batchSize=100, tolerance=0.001, learningRate=0.001}]
|===
--

The `parameterSpace` in the pipeline now contains the four different model candidates, expanded with the default values.
Each specified model candidate will be tried out during the model selection in xref:machine-learning/node-property-prediction/nodeclassification-pipelines/training.adoc[training].

[NOTE]
====
These are somewhat naive examples of how to add and configure model candidates.
Please see xref:machine-learning/training-methods/index.adoc[Training methods] for more information on how to tune the configuration parameters of each method.
====


[[nodeclassification-pipelines-configure-auto-tuning]]
== Configuring Auto-tuning

In order to find good models, the pipeline supports automatically tuning the parameters of the training algorithm.
Optionally, the procedure described below can be used to configure the auto-tuning behavior.
Otherwise, default auto-tuning configuration is used.
Currently, it is only possible to configure the maximum number trials of hyper-parameter settings which are evaluated.


=== Syntax

[.pipeline-configure-auto-tuning-syntax]
--
.Configuring auto-tuning syntax
[source, cypher, role=noplay]
----
CALL gds.alpha.pipeline.nodeClassification.configureAutoTuning(
  pipelineName: String,
  configuration: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  autoTuningConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type    | Description
| pipelineName    | String  | The name of the created pipeline.
| configuration   | Map     | The configuration for auto-tuning.
|===

.Configuration
[opts="header",cols="1,1,1,4"]
|===
| Name            | Type    | Default      | Description
| maxTrials       | Integer | {max-trials} | The value of `maxTrials` determines the maximum allowed model candidates that should be evaluated and compared when training the pipeline. If no ranges are present in the parameter space, `maxTrials` is ignored and the each model candidate in the parameter space is evaluated.
|===


include::partial$/machine-learning/node-property-prediction/pipelineInfoResult.adoc[]
--


=== Example

[role=query-example,group=nc]
--
.The following will configure the maximum trials for the auto-tuning:
[source, cypher, role=noplay]
----
CALL gds.alpha.pipeline.nodeClassification.configureAutoTuning('pipe', {
  maxTrials: 2
}) YIELD autoTuningConfig
----

.Results
[opts="header",cols="1"]
|===
| autoTuningConfig
| {maxTrials=2}
|===

We now reconfigured the auto-tuning to try out at most 100 model candidates during xref:machine-learning/node-property-prediction/nodeclassification-pipelines/training.adoc[training].
--
