== Class weights

This parameter introduces the concept of _class weights_, studied in 'Focal Loss for Dense Object Detection' by T. Lin et al.
It is often called _balanced cross entropy_.
It assigns a weight to each class in the cross-entropy loss function, thus allowing the model to treat different classes with varying importance.
It is defined for each example as:

image::equations/balanced-cross-entropy.svg[align="center"]

where `a~t~` denotes the class weight of the true class.`p~t~` denotes the probability of the true class.

For class-imbalanced problems, the class weights are often set to the inverse of class frequencies to improve the inductive bias of the model on minority classes.

=== Usage in link prediction
For link prediction, it must be a list of length 2 where the first weight is for negative examples (missing relationships) and the second for positive examples (actual relationships).

=== Usage in node classification
For node classification, the `i^th^` weight is for the `i^th^` class, ordered by the class values (which must be integers). For example, if your node classification dataset has three classes: 0, 1, 42. Then the class weights must be of length 3. The third weight is applied to class 42.


== Focus weight

This parameter introduces the concept of _focal loss_, again studied in 'Focal Loss for Dense Object Detection' by T. Lin et al.
When `focusWeight` is a value greater than zero, the loss function changes from standard Cross-Entropy Loss to Focal Loss.
It is defined for each example as:

image::equations/focal-loss.svg[align="center"]

where `p~t~` denotes the probability of the true class.
The `focusWeight` parameter is the exponent noted as `g`.

Increasing `focusWeight` will guide the model towards trying to fit "hard" misclassified examples.
A hard misclassified example is an example for which the model has a low predicted probability for the true class.
In the above equation, the loss will be exponentially higher for low-true-class-probability examples, thus tuning the model towards trying to fit them, at the expense of potentially being less confident on "easy" examples.

In class-imbalanced datasets, the minority class(es) are typically harder to classify correctly.
Read more about class imbalance for Link Prediction in xref:machine-learning/linkprediction-pipelines/theory.adoc#linkprediction-pipelines-classimbalance[Class Imbalance].